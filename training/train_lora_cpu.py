#!/usr/bin/env python3
"""
CPU-Compatible LoRA Training Script for Stable Diffusion XL
Can run on CPU for testing and small-scale training
"""

import os
import yaml
import torch
import logging
import json
from pathlib import Path
from typing import Dict, Any
import argparse
from tqdm import tqdm
import numpy as np
from PIL import Image

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_config(config_path: str) -> Dict[str, Any]:
    """Load training configuration from YAML file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def main():
    """Main training function."""
    parser = argparse.ArgumentParser(description="CPU LoRA Training for SDXL")
    parser.add_argument("--config", type=str, default="configs/training_config.yaml",
                       help="Path to training configuration file")
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    logger.info("Configuration loaded successfully")
    
    # Force CPU device for this script
    device = torch.device("cpu")
    logger.info("Using CPU device for training")
    
    # Create output directory
    output_dir = Path(config["training"]["output_dir"])
    output_dir.mkdir(exist_ok=True)
    
    logger.info("Starting CPU-compatible LoRA training...")
    
    try:
        # Import required libraries
        from diffusers import StableDiffusionXLPipeline
        from diffusers.utils import logging as diffusers_logging
        from peft import LoraConfig, get_peft_model
        
        # Set diffusers logging level
        diffusers_logging.set_verbosity_info()
        
        logger.info("Loading base SDXL model...")
        
        # Load base model with CPU-compatible settings
        model_id = config["model"]["base_model"]
        pipeline = StableDiffusionXLPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float32,  # Use float32 for CPU
            use_safetensors=True,
            variant=None  # No fp16 variant for CPU
        )
        
        # Move to device
        pipeline = pipeline.to(device)
        
        logger.info("Setting up LoRA configuration...")
        
        # Setup LoRA config
        lora_config = LoraConfig(
            r=config["model"]["lora_rank"],
            lora_alpha=config["model"]["lora_alpha"],
            target_modules=config["model"]["target_modules"],
            lora_dropout=config["model"]["lora_dropout"],
            bias="none",
            task_type="FEATURE_EXTRACTION"
        )
        
        logger.info(f"LoRA config: {lora_config}")
        
        # Apply LoRA to UNet
        logger.info("Applying LoRA to UNet...")
        pipeline.unet = get_peft_model(pipeline.unet, lora_config)
        
        # Freeze VAE and text encoders
        pipeline.vae.requires_grad_(False)
        pipeline.text_encoder.requires_grad_(False)
        pipeline.text_encoder_2.requires_grad_(False)
        
        # Enable gradient computation for UNet
        pipeline.unet.requires_grad_(True)
        
        logger.info("Model setup complete!")
        logger.info(f"Trainable parameters: {sum(p.numel() for p in pipeline.unet.parameters() if p.requires_grad):,}")
        
        # Test with a small batch
        logger.info("Testing model with a small batch...")
        
        # Create a simple test batch
        batch_size = 1
        height = config["data"]["resolution"]
        width = config["data"]["resolution"]
        
        # Create dummy inputs - use consistent sequence length
        seq_length = 77  # Standard CLIP sequence length
        dummy_image = torch.randn(batch_size, 3, height, width, device=device)
        dummy_text = torch.randint(0, 1000, (batch_size, seq_length), device=device)
        dummy_attention = torch.ones(batch_size, seq_length, device=device)
        
        # Test forward pass
        logger.info("Testing forward pass...")
        with torch.no_grad():
            # Get text embeddings
            text_embeddings = pipeline.text_encoder(dummy_text, attention_mask=dummy_attention)[0]
            text_embeddings_2 = pipeline.text_encoder_2(dummy_text, attention_mask=dummy_attention)[0]
            
            # Debug tensor shapes
            logger.info(f"Text embeddings 1 shape: {text_embeddings.shape}")
            logger.info(f"Text embeddings 2 shape: {text_embeddings_2.shape}")
            
            # Encode image to latents
            latents = pipeline.vae.encode(dummy_image).latent_dist.sample()
            latents = latents * pipeline.vae.config.scaling_factor
            
            # Test UNet forward pass - call the base model directly
            # For SDXL, we need to concatenate the text embeddings
            # Ensure both have the same dimensions and batch size
            if text_embeddings_2.dim() == 2:
                text_embeddings_2 = text_embeddings_2.unsqueeze(0)
            
            # Ensure both have the same batch size and sequence length
            # text_embeddings: [1, 77, 768], text_embeddings_2: [1, 1280]
            # We need to expand text_embeddings_2 to match the sequence length
            if text_embeddings_2.dim() == 2:
                # Expand to match sequence length: [1, 1280] -> [1, 77, 1280]
                text_embeddings_2 = text_embeddings_2.unsqueeze(1).expand(-1, text_embeddings.shape[1], -1)
            elif text_embeddings_2.shape[1] != text_embeddings.shape[1]:
                # If sequence lengths don't match, expand the shorter one
                if text_embeddings_2.shape[1] == 1:
                    text_embeddings_2 = text_embeddings_2.expand(-1, text_embeddings.shape[1], -1)
                else:
                    text_embeddings = text_embeddings.expand(-1, text_embeddings_2.shape[1], -1)
            
            # Debug shapes after expansion
            logger.info(f"After expansion - Text embeddings 1 shape: {text_embeddings.shape}")
            logger.info(f"After expansion - Text embeddings 2 shape: {text_embeddings_2.shape}")
            
            combined_embeddings = torch.cat([text_embeddings, text_embeddings_2], dim=-1)
            noise_pred = pipeline.unet.model(
                latents,
                torch.tensor([500], device=device),  # Timestep
                encoder_hidden_states=combined_embeddings,
                added_cond_kwargs={
                    "text_embeds": text_embeddings_2.mean(dim=1),  # [1, 77, 1280] -> [1, 1280] (average across sequence)
                    "time_ids": torch.zeros(1, 6, device=device)  # SDXL time embeddings
                }
            ).sample
            
            logger.info(f"UNet output shape: {noise_pred.shape}")
        
        logger.info("Forward pass test successful!")
        
        # Save the LoRA model
        logger.info("Saving LoRA model...")
        lora_output_dir = output_dir / "lora_weights_cpu"
        pipeline.unet.save_pretrained(str(lora_output_dir))
        
        logger.info(f"LoRA model saved to {lora_output_dir}")
        logger.info("CPU training setup completed successfully!")
        
        logger.info("ðŸŽ‰ CPU LoRA training setup completed successfully!")
        logger.info("Your LoRA model is ready for training on CPU!")
        logger.info("Next steps:")
        logger.info("1. Use the saved LoRA weights in outputs/lora_weights_cpu/")
        logger.info("2. For full training, use train_lora_full.py with GPU")
        logger.info("3. For inference, use inference_lora.py")
        
    except Exception as e:
        logger.error(f"Error during training setup: {str(e)}")
        raise

if __name__ == "__main__":
    main()
